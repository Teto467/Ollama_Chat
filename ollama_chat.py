import sys
import os
import ctypes
import msvcrt
import requests
import json
import gc
import psutil
import time
from typing import Optional
import concurrent.futures
from datetime import datetime
from itertools import cycle
import signal
from ctypes import wintypes
from tenacity import retry, stop_after_attempt, wait_exponential
from pynvml import (
    nvmlInit,
    nvmlDeviceGetHandleByIndex,
    nvmlDeviceGetMemoryInfo,
    nvmlDeviceGetName  # ã“ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
)

# Windows APIå®šç¾©
kernel32 = ctypes.windll.kernel32

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºç”¨
SPINNER = cycle(['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â '])

# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¨­å®š
kernel32.SetConsoleCP(65001)
kernel32.SetConsoleOutputCP(65001)

OLLAMA_API_URL = "http://localhost:11434"
COLOR = {
    "user": "\033[34m",  # é’
    "reset": "\033[0m",  # ãƒªã‚»ãƒƒãƒˆ
    "model": "\033[32m",  # ç·‘
    "number": "\033[33m",  # é»„
    "model_name": "\033[36m",  # ã‚·ã‚¢ãƒ³
    "date": "\033[35m",  # ãƒã‚¼ãƒ³ã‚¿
    "white": "\033[37m",  # ç™½ â† ã“ã“ã«ã‚«ãƒ³ãƒã‚’è¿½åŠ 
    "divider": "\033[90m",  # æ˜ã‚‹ã„ã‚°ãƒ¬ãƒ¼
    "highlight": "\033[1;36m"  # ã‚·ã‚¢ãƒ³ï¼‹å¤ªå­—
}

class TIME_ZONE_INFORMATION(ctypes.Structure):
    _fields_ = [
        ("Bias", ctypes.c_long),
        ("StandardName", ctypes.c_wchar * 32),  # c_wcharã«ä¿®æ­£
        ("StandardDate", ctypes.c_byte * 16),
        ("StandardBias", ctypes.c_long),
        ("DaylightName", ctypes.c_wchar * 32),  # c_wcharã«ä¿®æ­£
        ("DaylightDate", ctypes.c_byte * 16),
        ("DaylightBias", ctypes.c_long),
    ]

def clear_input_buffer():
    """Windowså°‚ç”¨å…¥åŠ›ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢"""
    try:
        while msvcrt.kbhit():
            msvcrt.getch()
    except Exception as e:
        print(f"å…¥åŠ›ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")

def get_local_timezone():
    """Windowsã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’æ­£ç¢ºã«å–å¾—"""
    tzi = TIME_ZONE_INFORMATION()
    if kernel32.GetTimeZoneInformation(ctypes.byref(tzi)) != 0xFFFFFFFF:
        return datetime.now().astimezone().tzinfo
    return datetime.utcnow().astimezone().tzinfo

def convert_to_local_time(utc_time):
    """UTCæ™‚åˆ»ã‚’ãƒ­ãƒ¼ã‚«ãƒ«æ™‚åˆ»ã«å¤‰æ›"""
    try:
        return utc_time.astimezone(get_local_timezone())
    except Exception as e:
        print(f"æ™‚åˆ»å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return utc_time

def get_models():
    """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—ï¼ˆæ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿®æ­£ç‰ˆï¼‰"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=15)
        response.raise_for_status()
        
        models = []
        for m in response.json().get("models", []):
            try:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãƒã‚¤ã‚¯ãƒ­ç§’ã‚’6æ¡ã«æ­£è¦åŒ–
                raw_time = m["modified_at"].rstrip('Z').replace('Z', '')
                if '.' in raw_time:
                    main_part, fractional = raw_time.split('.')
                    fractional = fractional.split('+')[0][:6]  # æœ€å¤§6æ¡ã«åˆ¶é™
                    tz_part = raw_time.split('+')[-1] if '+' in raw_time else ''
                    raw_time = f"{main_part}.{fractional}+{tz_part}" if tz_part else f"{main_part}.{fractional}"
                
                utc_time = datetime.fromisoformat(raw_time)
                models.append({
                    "name": m["name"],
                    "modified": convert_to_local_time(utc_time)
                })
            except Exception as e:
                print(f"ãƒ¢ãƒ‡ãƒ« {m['name']} ã®æ™‚åˆ»è§£æã«å¤±æ•—: {e}")
        
        return sorted(models, key=lambda x: x["modified"], reverse=True)
    except requests.exceptions.RequestException as e:
        print(f"ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def preload_model(model_name):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰"""
    global current_preload_model
    try:
        current_preload_model = model_name
        # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆä»®ã®å®Ÿè£…ï¼‰
        response = requests.post(
            f"{OLLAMA_API_URL}/api/load",
            json={"model": model_name, "keep_alive": "5m"}
        )
        response.raise_for_status()
        print(f"\n{COLOR['divider']}[âœ“] {model_name} ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†{COLOR['reset']}")
    except Exception as e:
        print(f"\n{COLOR['divider']}[!] ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}{COLOR['reset']}")
    finally:
        current_preload_model = None


def select_model(models):
    """ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆWindowsæœ€é©åŒ–ç‰ˆï¼‰"""
    # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
    header = (
        f"{COLOR['divider']}â”Œ{'â”€'*5}â”¬{'â”€'*25}â”¬{'â”€'*19}â”{COLOR['reset']}\n"
        f"{COLOR['number']}  No. {COLOR['divider']}â”‚{COLOR['model_name']} ãƒ¢ãƒ‡ãƒ«å{' '*18} "
        f"{COLOR['divider']}â”‚{COLOR['date']} æ›´æ–°æ—¥æ™‚{' '*11} {COLOR['divider']}â”‚{COLOR['reset']}"
    )
    
    # ãƒœãƒ‡ã‚£ä½œæˆ
    body = []
    for i, model in enumerate(models):
        time_str = model["modified"].strftime('%Y-%m-%d %H:%M')
        body_line = (
            f"{COLOR['divider']}â”œ{'â”€'*5}â”¼{'â”€'*25}â”¼{'â”€'*19}â”¤{COLOR['reset']}\n"
            f"{COLOR['number']}{i+1:>4}  {COLOR['divider']}â”‚ "
            f"{COLOR['model_name']}{model['name'][:23]:<23} {COLOR['divider']}â”‚ "
            f"{COLOR['date']}{time_str} {COLOR['divider']}â”‚"
        )
        body.append(body_line)
    
    # ãƒ•ãƒƒã‚¿ãƒ¼ä½œæˆ
    footer = f"{COLOR['divider']}â””{'â”€'*5}â”´{'â”€'*25}â”´{'â”€'*19}â”˜{COLOR['reset']}"
    
    # å…¨ä½“è¡¨ç¤º
    print(f"\n{header}\n" + "\n".join(body) + f"\n{footer}")
    
    # å…¥åŠ›å‡¦ç†
    while True:
        choice = safe_input(
            f"\n{COLOR['highlight']}ğŸ¡¢ {COLOR['white']}ãƒ¢ãƒ‡ãƒ«{COLOR['number']}No.{COLOR['white']}ã‚’å…¥åŠ›"
            f"{COLOR['divider']} [0:çµ‚äº†] {COLOR['reset']}"
        ).strip()
        
        if choice in ("0", "/exit"):
            print(f"{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•{COLOR['reset']}")
            print("ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
            exit()
        if choice.isdigit() and 1 <= int(choice) <= len(models):
            return models[int(choice)-1]["name"]
        
        print(
            f"{COLOR['divider']}[!] {COLOR['number']}1ã€œ{len(models)}ã®æ•°å€¤ã§å…¥åŠ›ã—ã¦ãã ã•ã„"
            f"{COLOR['reset']}"
        )

def safe_input(prompt):
    """Windowså‘ã‘å¼·åŒ–ç‰ˆå…¥åŠ›å‡¦ç†ï¼ˆã‚·ã‚°ãƒŠãƒ«å¯¾å¿œï¼‰"""
    h_input = kernel32.GetStdHandle(-10)
    original_mode = wintypes.DWORD()
    kernel32.GetConsoleMode(h_input, ctypes.byref(original_mode))
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ã‚’ä¿å­˜
    original_sigint = signal.getsignal(signal.SIGINT)
    buf = ctypes.create_unicode_buffer(256)
    received_signal = [False]

    def handler(signum, frame):
        received_signal[0] = True
        print("\nä¸­æ–­ä¿¡å·ã‚’æ¤œçŸ¥")

    try:
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
        signal.signal(signal.SIGINT, handler)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        new_mode = original_mode.value | 0x0002 | 0x0004 | 0x0001  # ENABLE flags
        kernel32.SetConsoleMode(h_input, new_mode)
        
        print(prompt, end='', flush=True)
        chars_read = wintypes.DWORD()
        
        # å…¥åŠ›ã‚’éåŒæœŸã§ç›£è¦–
        while not received_signal[0]:
            if kernel32.WaitForSingleObject(h_input, 100) == 0:
                if kernel32.ReadConsoleW(h_input, buf, len(buf)-1, ctypes.byref(chars_read), None):
                    return buf.value[:chars_read.value].strip()
                break
        return ""
    
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
        kernel32.SetConsoleMode(h_input, original_mode)
        signal.signal(signal.SIGINT, original_sigint)
        while msvcrt.kbhit():
            msvcrt.getwch()
        if received_signal[0]:
            raise KeyboardInterrupt("å…¥åŠ›ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šä¸­æ–­ã•ã‚Œã¾ã—ãŸ")

def monitor_resources():
    """GPU/CPUãƒªã‚½ãƒ¼ã‚¹ç›£è¦–"""
    nvmlInit()
    handle = nvmlDeviceGetHandleByIndex(0)
    gpu_info = nvmlDeviceGetMemoryInfo(handle)
    
    process = psutil.Process(os.getpid())
    cpu_usage = process.cpu_percent(interval=1)
    mem_usage = process.memory_info().rss / 1024 ** 3  # GBå˜ä½
    
    print(f" [GPU: {gpu_info.used/1024**2:.1f}MB | CPU: {cpu_usage}% | RAM: {mem_usage:.2f}GB]")


def initialize_gpu_resources() -> dict:
    """GPUãƒªã‚½ãƒ¼ã‚¹ã®åˆæœŸåŒ–ã¨è¨­å®šã‚’ç®¡ç†"""
    gpu_context = {
        'use_cuda': False,
        'torch': None,
        'handle': None
    }
    
    try:
        import torch
        from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetName, nvmlDeviceGetMemoryInfo
        
        nvmlInit()
        if torch.cuda.is_available():
            device = torch.device("cuda")
            torch.cuda.init()
            
            # CUDAæœ€é©åŒ–è¨­å®š
            torch.cuda.set_per_process_memory_fraction(0.9, device=0)
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
            
            # GPUãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—
            handle = nvmlDeviceGetHandleByIndex(0)
            gpu_name = nvmlDeviceGetName(handle)
            gpu_mem = nvmlDeviceGetMemoryInfo(handle)
            
            print(f"{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•{COLOR['reset']}")
            print(f"{COLOR['model']}âš¡ CUDAæœ‰åŠ¹: {gpu_name} [VRAM: {gpu_mem.free/1024**3:.1f}GB ç©ºã]{COLOR['reset']}")
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
            
            gpu_context.update({
                'use_cuda': True,
                'torch': torch,
                'handle': handle
            })
            
    except Exception as e:
        print(f"{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        print(f"{COLOR['model']}âš  GPUåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}{COLOR['reset']}")
    
    return gpu_context

def set_high_process_priority():
    """Windowsãƒ—ãƒ­ã‚»ã‚¹ã®å„ªå…ˆåº¦ã‚’è¨­å®š"""
    kernel32.SetPriorityClass(kernel32.GetCurrentProcess(), 0x00000080)

def create_request_payload(model: str, prompt: str, use_cuda: bool) -> dict:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
    return {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": True,
        "options": {
            "num_ctx": 4096,
            "num_thread": 8 if use_cuda else os.cpu_count(),
            "num_batch": 512,
            "flash_attention": True
        }
    }

def handle_chat_response(response: requests.Response, model: str, executor: concurrent.futures.Executor, gpu_context: dict):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ã¨éåŒæœŸã‚¿ã‚¹ã‚¯ç®¡ç†"""
    future = None
    try:
        print(f"{COLOR['model']}{model}: ", end="", flush=True)
        future = executor.submit(process_stream, response, model)
        
        while not future.done():
            if gpu_context['use_cuda']:
                kernel32.SetProcessWorkingSetSize(-1, 1024*1024*1024, -1)
                gpu_context['torch'].cuda.empty_cache()
            time.sleep(0.05)
            
        future.result()
        print(f"\n{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        
    finally:
        if future:
            future.cancel()

def run_chat_loop(model: str, gpu_context: dict):
    """ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
    with concurrent.futures.ThreadPoolExecutor(
        max_workers=2,
        thread_name_prefix='OllamaStream'
    ) as executor:
        while True:
            try:
                prompt = safe_input(f"{COLOR['user']}ã‚ãªãŸ: {COLOR['reset']}").strip()
                if not prompt or prompt.lower() == "/exit":
                    return

                payload = create_request_payload(model, prompt, gpu_context['use_cuda'])
                
                with requests.post(
                    f"{OLLAMA_API_URL}/api/chat",
                    json=payload,
                    stream=True,
                    timeout=150
                ) as response:
                    response.raise_for_status()
                    handle_chat_response(response, model, executor, gpu_context)
                
                if gpu_context['use_cuda']:
                    gpu_context['torch'].cuda.empty_cache()
                gc.collect()
                
            except KeyboardInterrupt:
                print(f"{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
                print(f"{COLOR['reset']}\nå…¥åŠ›ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
                break
            except requests.exceptions.RequestException as e:
                print(f"\n{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
                print(f"é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

def cleanup_resources(response: Optional[requests.Response], gpu_context: dict):
    """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
    if response:
        response.close()
    if gpu_context['torch'] is not None and gpu_context['use_cuda']:
        gpu_context['torch'].cuda.synchronize()
        gpu_context['torch'].cuda.empty_cache()
        gpu_context['torch'].cuda.reset_peak_memory_stats()
    kernel32.SetPriorityClass(kernel32.GetCurrentProcess(), 0x00000020)

def chat_session(model: str):
    """åˆ†å‰²å¾Œã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    response = None
    gpu_context = initialize_gpu_resources()
    
    try:
        set_high_process_priority()
        print(f"\n{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        print(f"{COLOR['model_name']} ãƒãƒ£ãƒƒãƒˆé–‹å§‹: {model}{COLOR['reset']}")
        print(f"{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•{COLOR['reset']}\n")
        
        run_chat_loop(model, gpu_context)
        
    except Exception as e:
        print(f"\n{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        print(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        cleanup_resources(response, gpu_context)
        print(f"{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        print(f"{COLOR['reset']}ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã™\n" + "="*60)


def process_stream(response, model):
    """æœªåŠ å·¥ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›å‡¦ç†"""
    buffer = bytearray()
    token_count = 0
    start_time = time.perf_counter()
    
    try:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                buffer.extend(chunk)
                while b'\n' in buffer:
                    line, _, buffer = buffer.partition(b'\n')
                    if not line:
                        continue
                    
                    try:
                        data = json.loads(line)
                        content = data.get("message", {}).get("content", "")
                        # æœªåŠ å·¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãã®ã¾ã¾å‡ºåŠ›
                        print(content, end="", flush=True)
                        token_count += len(content.split())
                            
                    except json.JSONDecodeError:
                        continue
                    
        elapsed = time.perf_counter() - start_time
        tps = token_count / elapsed if elapsed > 0 else 0
        print(f"\n{COLOR['divider']}â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        print(f"{COLOR['number']}âš¡ å‡¦ç†é€Ÿåº¦: {tps:.1f} tokens/sec")
        
    except Exception as e:
        print(f"\nã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    return buffer

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆWindowså°‚ç”¨ç‰ˆï¼‰"""
    try:
        while True:
            models = get_models()
            if not models:
                print("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                if input("å†è©¦è¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower() != 'y':
                    return
                continue
            
            try:
                model = select_model(models)
                chat_session(model)
            except KeyboardInterrupt:
                print("\nãƒ¢ãƒ‡ãƒ«é¸æŠã«æˆ»ã‚Šã¾ã™")
    except KeyboardInterrupt:
        print("\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")

if __name__ == "__main__":
    main()